{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dogImages/train/095.Kuvasz/Kuvasz_06442.jpg',\n",
       "       'dogImages/train/057.Dalmatian/Dalmatian_04054.jpg',\n",
       "       'dogImages/train/088.Irish_water_spaniel/Irish_water_spaniel_06014.jpg',\n",
       "       'dogImages/train/008.American_staffordshire_terrier/American_staffordshire_terrier_00596.jpg',\n",
       "       'dogImages/train/008.American_staffordshire_terrier/American_staffordshire_terrier_00563.jpg'], \n",
       "      dtype='<U99')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np_utils.to_categorical([0,1,2,3,3], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Human face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "# import human dataset\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def plot_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    # convert BGR image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    # get bounding box for each detected face\n",
    "    for (x,y,w,h) in faces:\n",
    "        # add bounding box to color image\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "    # convert BGR image to RGB for plotting\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(cv_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_image_plain(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    # convert BGR image to RGB for plotting\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(cv_rgb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try using resnet\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert and load images\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [01:02<00:00, 107.70it/s]\n",
      "100%|██████████| 835/835 [00:07<00:00, 111.01it/s]\n",
      "100%|██████████| 836/836 [00:07<00:00, 111.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# load and preprocess data\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess image for using with imagenet predictions\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nb_train_samples = 20\n",
    "nb_validation_samples = 5\n",
    "batch_size = 3\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "train_batch = batch(train_tensors, batch_size) \n",
    "valid_batch = batch(valid_tensors, batch_size)\n",
    "test_batch = batch(test_tensors, batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# build the resnet50 network\n",
    "resnet50_model = ResNet50(include_top=False, weights='imagenet')\n",
    "#bottleneck_features_train = model.predict_generator(train_batch, nb_train_samples // batch_size)\n",
    "bottleneck_features_train = resnet50_model.predict(preprocess_input(train_tensors))\n",
    "np.save(open('bottleneck_features_train.npy', 'wb'),bottleneck_features_train)\n",
    "\n",
    "#bottleneck_features_validation = model.predict_generator(valid_batch, nb_validation_samples // batch_size)\n",
    "bottleneck_features_validation = resnet50_model.predict(preprocess_input(valid_tensors))\n",
    "np.save(open('bottleneck_features_validation.npy', 'wb'),bottleneck_features_validation)\n",
    "\n",
    "#bottleneck_features_test = model.predict_generator(test_batch, nb_validation_samples // batch_size)\n",
    "bottleneck_features_test = resnet50_model.predict(preprocess_input(test_tensors))\n",
    "np.save(open('bottleneck_features_test.npy', 'wb'),bottleneck_features_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "bnf_train_name = 'bottleneck_features_inceptionv3_train.npy'\n",
    "bnf_test_name = 'bottleneck_features_inceptionv3_test.npy' \n",
    "bnf_valid_name = 'bottleneck_features_inceptionv3_valid.npy'\n",
    "\n",
    "# build the inceptionv3 network\n",
    "inceptionv3_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "#bottleneck_features_train = model.predict_generator(train_batch, nb_train_samples // batch_size)\n",
    "bottleneck_features_train = inceptionv3_model.predict(preprocess_input(train_tensors))\n",
    "np.save(open(bnf_train_name, 'wb'),bottleneck_features_train)\n",
    "\n",
    "#bottleneck_features_validation = model.predict_generator(valid_batch, nb_validation_samples // batch_size)\n",
    "bottleneck_features_validation = inceptionv3_model.predict(preprocess_input(valid_tensors))\n",
    "np.save(open(bnf_test_name, 'wb'),bottleneck_features_validation)\n",
    "\n",
    "#bottleneck_features_test = model.predict_generator(test_batch, nb_validation_samples // batch_size)\n",
    "bottleneck_features_test = inceptionv3_model.predict(preprocess_input(test_tensors))\n",
    "np.save(open(bnf_valid_name, 'wb'),bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# check if running on gpu, method1\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check if running on gpu, method2\n",
    "with tf.device('/cpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load convered data back\n",
    "train_data = np.load(open(bnf_train_name, 'rb'))\n",
    "train_labels = train_targets[:train_data.shape[0]]\n",
    "validation_data = np.load(open(bnf_test_name, 'rb'))\n",
    "validation_labels = valid_targets[:validation_data.shape[0]]\n",
    "test_data = np.load(open(bnf_valid_name, 'rb'))\n",
    "test_labels = test_targets[:test_data.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6680, 224, 224, 3) (6680, 133)\n",
      "(6680, 5, 5, 2048) (6680, 133)\n",
      "(835, 5, 5, 2048) (835, 133)\n",
      "(836, 5, 5, 2048) (836, 133)\n"
     ]
    }
   ],
   "source": [
    "print(train_tensors.shape, train_targets.shape)\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(validation_data.shape, validation_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Check model\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogInceptionV3Data.npz')\n",
    "train_inception_v3 = bottleneck_features['train']\n",
    "valid_inception_v3 = bottleneck_features['valid']\n",
    "test_inception_v3 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print(train_tensors.shape, train_targets.shape)\n",
    "print(train_inception_v3.shape, train_labels.shape)\n",
    "print(valid_inception_v3.shape, validation_labels.shape)\n",
    "print(test_inception_v3.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(valid_inception_v3[1,0,0, :5])\n",
    "print(validation_data[1,0,0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_13  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 1,117,317.0\n",
      "Trainable params: 1,117,317.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(GlobalAveragePooling2D(input_shape=train_data.shape[1:]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 50 samples\n",
      "Epoch 1/25\n",
      "150/150 [==============================] - 1s - loss: 5.6757 - acc: 0.0067 - val_loss: 4.7278 - val_acc: 0.0600\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 0s - loss: 4.2157 - acc: 0.1267 - val_loss: 4.5237 - val_acc: 0.1400\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 0s - loss: 3.0877 - acc: 0.3333 - val_loss: 4.3090 - val_acc: 0.2000\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 0s - loss: 2.1835 - acc: 0.5200 - val_loss: 4.0898 - val_acc: 0.3000\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 0s - loss: 1.5300 - acc: 0.6800 - val_loss: 3.9668 - val_acc: 0.3000\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 0s - loss: 1.0697 - acc: 0.7400 - val_loss: 3.8658 - val_acc: 0.3400\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 0s - loss: 0.6570 - acc: 0.8800 - val_loss: 3.7809 - val_acc: 0.3800\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 0s - loss: 0.4060 - acc: 0.9333 - val_loss: 3.7943 - val_acc: 0.4000\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 0s - loss: 0.2248 - acc: 0.9667 - val_loss: 3.8965 - val_acc: 0.4000\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 0s - loss: 0.1839 - acc: 0.9600 - val_loss: 3.9886 - val_acc: 0.4600\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 0s - loss: 0.1718 - acc: 0.9667 - val_loss: 4.0495 - val_acc: 0.5000\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 0s - loss: 0.1233 - acc: 0.9667 - val_loss: 4.0985 - val_acc: 0.5000\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbab277cfd0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.mymodel.resnet50.hdf5', verbose=1, save_best_only=True)\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.mymodel.inceptionv3.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(train_data[:150], train_labels[:150],\n",
    "          epochs=25,\n",
    "          batch_size=64,\n",
    "          validation_data=(validation_data[:50], validation_labels[:50]),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.3333%\n"
     ]
    }
   ],
   "source": [
    "### train accuracy\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in train_data[:150]]\n",
    "\n",
    "# report test accuracy\n",
    "train_accuracy = 100*np.sum(np.array(predictions)==np.argmax(train_labels[:150], axis=1))/len(predictions)\n",
    "print('Train accuracy: %.4f%%' % train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 44.0000%\n"
     ]
    }
   ],
   "source": [
    "### test accuracy\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_data[:50]]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_labels[:50], axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/25\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 1.6492 - acc: 0.6306Epoch 00000: val_loss improved from inf to 0.66865, saving model to saved_models/weights.best.mymodel.inceptionv3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.6465 - acc: 0.6311 - val_loss: 0.6686 - val_acc: 0.8012\n",
      "Epoch 2/25\n",
      "6528/6680 [============================>.] - ETA: 0s - loss: 0.7292 - acc: 0.7832Epoch 00001: val_loss improved from 0.66865 to 0.61160, saving model to saved_models/weights.best.mymodel.inceptionv3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.7307 - acc: 0.7825 - val_loss: 0.6116 - val_acc: 0.8204\n",
      "Epoch 3/25\n",
      "6592/6680 [============================>.] - ETA: 0s - loss: 0.5851 - acc: 0.8195Epoch 00002: val_loss improved from 0.61160 to 0.57022, saving model to saved_models/weights.best.mymodel.inceptionv3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.5846 - acc: 0.8201 - val_loss: 0.5702 - val_acc: 0.8323\n",
      "Epoch 4/25\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8386Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.5107 - acc: 0.8386 - val_loss: 0.5770 - val_acc: 0.8419\n",
      "Epoch 5/25\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8579Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.4482 - acc: 0.8573 - val_loss: 0.5747 - val_acc: 0.8180\n",
      "Epoch 6/25\n",
      "6592/6680 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8676Epoch 00005: val_loss improved from 0.57022 to 0.55959, saving model to saved_models/weights.best.mymodel.inceptionv3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.3948 - acc: 0.8672 - val_loss: 0.5596 - val_acc: 0.8347\n",
      "Epoch 7/25\n",
      "6592/6680 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8838Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3505 - acc: 0.8834 - val_loss: 0.6051 - val_acc: 0.8240\n",
      "Epoch 8/25\n",
      "6592/6680 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8876Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3329 - acc: 0.8879 - val_loss: 0.5651 - val_acc: 0.8407\n",
      "Epoch 9/25\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.8980Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.3058 - acc: 0.8981 - val_loss: 0.5836 - val_acc: 0.8479\n",
      "Epoch 10/25\n",
      "6592/6680 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9000Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2864 - acc: 0.8996 - val_loss: 0.6145 - val_acc: 0.8419\n",
      "Epoch 11/25\n",
      "6528/6680 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9124Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.2669 - acc: 0.9121 - val_loss: 0.6376 - val_acc: 0.8347\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb300b2da0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.mymodel.resnet50.hdf5', verbose=1, save_best_only=True)\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.mymodel.inceptionv3.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=25,\n",
    "          batch_size=64,\n",
    "          validation_data=(validation_data, validation_labels),\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 96.6667%\n"
     ]
    }
   ],
   "source": [
    "### train accuracy\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in train_data[:150]]\n",
    "\n",
    "# report test accuracy\n",
    "train_accuracy = 100*np.sum(np.array(predictions)==np.argmax(train_labels[:150], axis=1))/len(predictions)\n",
    "print('Train accuracy: %.4f%%' % train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.4689%\n"
     ]
    }
   ],
   "source": [
    "### test accuracy\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_labels, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_Resnet50(tensor):\n",
    "\tfrom keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\treturn ResNet50(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n",
    "\n",
    "def extract_InceptionV3(tensor):\n",
    "\tfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\treturn InceptionV3(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n",
    "\n",
    "def extract(tensor, model, preprocess_input):\n",
    "    return model.predict(preprocess_input(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input as inception_v3_preprocess_input\n",
    "def predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    #bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    bottleneck_feature = extract(path_to_tensor(img_path), inceptionv3_model, inception_v3_preprocess_input)\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_feature = extract(path_to_tensor(\"trisha.jpg\"), inceptionv3_model, inception_v3_preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_feature = path_to_tensor(\"trisha.jpg\")\n",
    "bottleneck_feature.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finnish_spitz'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict_breed(\"lfw/Jennifer_Renee_Short/Jennifer_Renee_Short_0001.jpg\")\n",
    "predict_breed(\"trisha.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_breed(img_path):\n",
    "    is_dog = False\n",
    "    is_human = False\n",
    "    if dog_detector(img_path):\n",
    "        is_dog = True\n",
    "    elif face_detector(img_path):\n",
    "        is_human = True\n",
    "    else:\n",
    "        print(\"No dog or humans found\")\n",
    "        plot_image_plain(img_path)\n",
    "        return\n",
    "    dog_breed = predict_breed(img_path)\n",
    "    if(is_human):\n",
    "        print(\"Hey there...\")\n",
    "        plot_image_plain(img_path)\n",
    "        print(\"You look like a...\\n {dog_breed}\".format(dog_breed=dog_breed))\n",
    "    else:\n",
    "        plot_image_plain(img_path)\n",
    "        print(\"This look like a...\\n {dog_breed}\".format(dog_breed=dog_breed))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 0.0002300739288330078\n"
     ]
    }
   ],
   "source": [
    "#detect_breed(\"lfw/Jennifer_Renee_Short/Jennifer_Renee_Short_0001.jpg\")\n",
    "#detect_breed(\"dhana.jpg\")\n",
    "import time\n",
    "t0 = time.time()\n",
    "for i in range(1, 2):\n",
    "    detect_breed(\"nivetha.jpg\")\n",
    "print(\"time taken\", time.time()-t0)\n",
    "#detect_breed(\"cat.jpg\")\n",
    "#detect_breed(\"trisha.jpg\")\n",
    "#detect_breed(\"sara.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
